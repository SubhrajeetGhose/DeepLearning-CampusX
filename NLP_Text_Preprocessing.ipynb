{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9IBmwvNByMVWczRVUxPIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SubhrajeetGhose/DeepLearning-CampusX/blob/main/NLP_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing HTML Tags"
      ],
      "metadata": {
        "id": "T41XoZ39pykR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text='<h1>This is heading 1</h1>\\n<h2>This is heading 2</h2>\\n<h3>This is heading 3</h3>'"
      ],
      "metadata": {
        "id": "Xa1QZpajpRSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BDKadIa5pRVg",
        "outputId": "16c188e5-ec24-4336-9f67-2b3c9626f0d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<h1>This is heading 1</h1>\\n<h2>This is heading 2</h2>\\n<h3>This is heading 3</h3>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def striphtml(data):\n",
        "  p=re.compile(r'<.*?>')\n",
        "  return p.sub('',data)"
      ],
      "metadata": {
        "id": "nT4yOINLpRYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "striphtml(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uJcp_v4vpRbk",
        "outputId": "31e088e8-b2dd-4b96-e9f9-314a37a1fa3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is heading 1\\nThis is heading 2\\nThis is heading 3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove URL"
      ],
      "metadata": {
        "id": "uqKAZkLX-Ro6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url(text):\n",
        "  pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return pattern.sub(r'',text)"
      ],
      "metadata": {
        "id": "YQU3lSi2-Uaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1= 'Check out my notebook https://www.udemy.com/course'"
      ],
      "metadata": {
        "id": "WTH0EXhs-ga6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_url(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TltqcshV-tWO",
        "outputId": "6c4e5e35-8862-417f-cdb2-cbd79127b2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Check out my notebook '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Punctuation"
      ],
      "metadata": {
        "id": "gITRqrwJ_yMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1"
      ],
      "metadata": {
        "id": "pyGj-FVWApaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string, time\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PtSz5Y1A_1AC",
        "outputId": "3952209c-b414-4077-9000-3477be0b8337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_exclude_punc = string.punctuation"
      ],
      "metadata": {
        "id": "Qz2FUH4u_-dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(text):\n",
        "  for char in to_exclude_punc:\n",
        "    text=text.replace(char,'')\n",
        "  return text"
      ],
      "metadata": {
        "id": "jxo-p8aU_-mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' String. with ,? punctuation!!!'"
      ],
      "metadata": {
        "id": "yXaWtEP5AKVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punc(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e0fzvx7JAKYt",
        "outputId": "3205cbab-d201-4ef7-8d96-4be2eda7ca62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' String with  punctuation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2\n",
        "- Faster and better"
      ],
      "metadata": {
        "id": "mIHuwY3SAtaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc1(text):\n",
        "  return text.translate(str.maketrans('','',string.punctuation))"
      ],
      "metadata": {
        "id": "JK85HByeAvcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_punc1(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xVBQ5hG8A8Kt",
        "outputId": "8cb3c592-581b-4efb-9a0b-0b83216570b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' String with  punctuation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat word treatment"
      ],
      "metadata": {
        "id": "1r11qhqJBu_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words={\n",
        "    'AFAIK':'As Far As I Know',\n",
        "    'AFK':'Away From Keyboard',\n",
        "    'ASAP':'As Soon As Possible',\n",
        "    'ATK':'At The Keyboard',\n",
        "    'ATM':'At The Moment',\n",
        "    'A3':'Anytime, Anywhere, Anyplace',\n",
        "    'BAK':'Back At Keyboard',\n",
        "    'BBL':'Be Back Later',\n",
        "    'BBS':'Be Back Soon',\n",
        "    'BFN':'Bye For Now',\n",
        "    'B4N':'Bye For Now',\n",
        "    'BRB':'Be Right Back',\n",
        "    'BRT':'Be Right There',\n",
        "    'BTW':'By The Way',\n",
        "    'B4':'Before',\n",
        "    'B4N':'Bye For Now',\n",
        "    'CU':'See You',\n",
        "    'CUL8R':'See You Later',\n",
        "    'CYA':'See You',\n",
        "    'FAQ':'Frequently Asked Questions',\n",
        "    'FC':'Fingers Crossed',\n",
        "    'FWIW':'For What It\\'s Worth',\n",
        "    'FYI':'For Your Information',\n",
        "    'GAL':'Get A Life',\n",
        "    'GG':'Good Game',\n",
        "    'GN':'Good Night',\n",
        "    'GMTA':'Great Minds Think Alike',\n",
        "    'GR8':'Great!',\n",
        "    'G9':'Genius',\n",
        "    'IC':'I See',\n",
        "    'ICQ':'I Seek you',\n",
        "    'ILU':'I Love You',\n",
        "    'IMHO':'In My Honest/Humble Opinion',\n",
        "    'IMO':'In My Opinion',\n",
        "    'IOW':'In Other Words',\n",
        "    'IRL':'In Real Life',\n",
        "    'KISS':'Keep It Simple, Stupid',\n",
        "    'LDR':'Long Distance Relationship',\n",
        "    'LMAO':'Laugh My A.. Off',\n",
        "    'LOL':'Laughing Out Loud',\n",
        "    'LTNS':'Long Time No See',\n",
        "    'L8R':'Later',\n",
        "    'MTE':'My Thoughts Exactly',\n",
        "    'M8':'Mate',\n",
        "    'NRN':'No Reply Necessary',\n",
        "    'OIC':'Oh I See',\n",
        "    'OMG':'Oh My God',\n",
        "    'PITA':'Pain In The Ass',\n",
        "    'PRT':'Party',\n",
        "    'PRW':'Parents Are Watching',\n",
        "    'QPSA':'Que Pasa?',\n",
        "    'ROFL':'Rolling On The Floor Laughing'\n",
        "}"
      ],
      "metadata": {
        "id": "hzMVmsLNBsr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8VhMOhABsvP",
        "outputId": "0b3dff74-fa9e-43e7-d0bc-c880868e8132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AFAIK': 'As Far As I Know',\n",
              " 'AFK': 'Away From Keyboard',\n",
              " 'ASAP': 'As Soon As Possible',\n",
              " 'ATK': 'At The Keyboard',\n",
              " 'ATM': 'At The Moment',\n",
              " 'A3': 'Anytime, Anywhere, Anyplace',\n",
              " 'BAK': 'Back At Keyboard',\n",
              " 'BBL': 'Be Back Later',\n",
              " 'BBS': 'Be Back Soon',\n",
              " 'BFN': 'Bye For Now',\n",
              " 'B4N': 'Bye For Now',\n",
              " 'BRB': 'Be Right Back',\n",
              " 'BRT': 'Be Right There',\n",
              " 'BTW': 'By The Way',\n",
              " 'B4': 'Before',\n",
              " 'CU': 'See You',\n",
              " 'CUL8R': 'See You Later',\n",
              " 'CYA': 'See You',\n",
              " 'FAQ': 'Frequently Asked Questions',\n",
              " 'FC': 'Fingers Crossed',\n",
              " 'FWIW': \"For What It's Worth\",\n",
              " 'FYI': 'For Your Information',\n",
              " 'GAL': 'Get A Life',\n",
              " 'GG': 'Good Game',\n",
              " 'GN': 'Good Night',\n",
              " 'GMTA': 'Great Minds Think Alike',\n",
              " 'GR8': 'Great!',\n",
              " 'G9': 'Genius',\n",
              " 'IC': 'I See',\n",
              " 'ICQ': 'I Seek you',\n",
              " 'ILU': 'I Love You',\n",
              " 'IMHO': 'In My Honest/Humble Opinion',\n",
              " 'IMO': 'In My Opinion',\n",
              " 'IOW': 'In Other Words',\n",
              " 'IRL': 'In Real Life',\n",
              " 'KISS': 'Keep It Simple, Stupid',\n",
              " 'LDR': 'Long Distance Relationship',\n",
              " 'LMAO': 'Laugh My A.. Off',\n",
              " 'LOL': 'Laughing Out Loud',\n",
              " 'LTNS': 'Long Time No See',\n",
              " 'L8R': 'Later',\n",
              " 'MTE': 'My Thoughts Exactly',\n",
              " 'M8': 'Mate',\n",
              " 'NRN': 'No Reply Necessary',\n",
              " 'OIC': 'Oh I See',\n",
              " 'OMG': 'Oh My God',\n",
              " 'PITA': 'Pain In The Ass',\n",
              " 'PRT': 'Party',\n",
              " 'PRW': 'Parents Are Watching',\n",
              " 'QPSA': 'Que Pasa?',\n",
              " 'ROFL': 'Rolling On The Floor Laughing'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_conversion(text):\n",
        "  new_text=[]\n",
        "  for w in text.split():\n",
        "    if w.upper() in chat_words:\n",
        "      new_text.append(chat_words[w.upper()])\n",
        "    else:\n",
        "      new_text.append(w)\n",
        "  return \" \".join(new_text)"
      ],
      "metadata": {
        "id": "4nC1edwQBszA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_conversion(' IMHO he is the best')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-FmZ7AlTCnEP",
        "outputId": "fd9b18a5-8679-4adc-81a8-1ae3fad993a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In My Honest/Humble Opinion he is the best'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unicode Normalisation"
      ],
      "metadata": {
        "id": "-32ZgUmlqZOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_text=\"In my professional journey, Iâ€™ve worked on exciting projects that combined data analysis and creative problem-solving ðŸš€. At Deloitte, I helped reduce the attrition rate by 26% ðŸ“‰ through insights-driven dashboards built with SQL and Tableau, empowering HR teams to make smarter decisions ðŸ¤. At Target ðŸŽ¯, I contributed to key deliverables using Python, Hive, and Excel, showcasing my knack for building efficient workflows and insightful data models ðŸ“Š. Additionally, I spearheaded a project to analyze driver performance ðŸš›, optimizing metrics like distance covered and drive time, which led to a 15% cost saving in the next quarter ðŸ’°. My passion for turning data into actionable insights fuels my drive to keep delivering impactful results! ðŸŒŸ\""
      ],
      "metadata": {
        "id": "DchBRU5opRe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "DmczEt-gpRhb",
        "outputId": "6d702ca6-11dc-4f89-a555-e84bf3155701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In my professional journey, Iâ€™ve worked on exciting projects that combined data analysis and creative problem-solving ðŸš€. At Deloitte, I helped reduce the attrition rate by 26% ðŸ“‰ through insights-driven dashboards built with SQL and Tableau, empowering HR teams to make smarter decisions ðŸ¤. At Target ðŸŽ¯, I contributed to key deliverables using Python, Hive, and Excel, showcasing my knack for building efficient workflows and insightful data models ðŸ“Š. Additionally, I spearheaded a project to analyze driver performance ðŸš›, optimizing metrics like distance covered and drive time, which led to a 15% cost saving in the next quarter ðŸ’°. My passion for turning data into actionable insights fuels my drive to keep delivering impactful results! ðŸŒŸ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_text.encode('utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTLnu315xU8V",
        "outputId": "197e2734-7bbc-433d-8829-8ec3135e2ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'In my professional journey, I\\xe2\\x80\\x99ve worked on exciting projects that combined data analysis and creative problem-solving \\xf0\\x9f\\x9a\\x80. At Deloitte, I helped reduce the attrition rate by 26% \\xf0\\x9f\\x93\\x89 through insights-driven dashboards built with SQL and Tableau, empowering HR teams to make smarter decisions \\xf0\\x9f\\xa4\\x9d. At Target \\xf0\\x9f\\x8e\\xaf, I contributed to key deliverables using Python, Hive, and Excel, showcasing my knack for building efficient workflows and insightful data models \\xf0\\x9f\\x93\\x8a. Additionally, I spearheaded a project to analyze driver performance \\xf0\\x9f\\x9a\\x9b, optimizing metrics like distance covered and drive time, which led to a 15% cost saving in the next quarter \\xf0\\x9f\\x92\\xb0. My passion for turning data into actionable insights fuels my drive to keep delivering impactful results! \\xf0\\x9f\\x8c\\x9f'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spell Checking"
      ],
      "metadata": {
        "id": "lOLnwzGWxi9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_text= ' ceertain conditionas durring seveal ggenerations aree moodified in the saame maner.'"
      ],
      "metadata": {
        "id": "x_EWGCePxVE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "textBlb=TextBlob(incorrect_text)\n",
        "\n",
        "textBlb.correct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8lAdVGxo4w2",
        "outputId": "dec444e8-8511-4ccd-fad3-57e6827cefcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\" certain conditions during several generations are modified in the same manner.\")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Stopwords"
      ],
      "metadata": {
        "id": "R-XLBg40DjY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzpSh1hIDjm8",
        "outputId": "0a169c63-6d19-4779-8a74-671415c5ced4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glIRpFjxDjp4",
        "outputId": "0512449a-4433-45a0-fcdd-a858829f5d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  new_text=[]\n",
        "  for word in text.split():\n",
        "    if word in stopwords.words('english'):\n",
        "      new_text.append('')\n",
        "    else:\n",
        "      new_text.append(word)\n",
        "  x=new_text[:]\n",
        "  new_text.clear()\n",
        "  return \" \".join(x)"
      ],
      "metadata": {
        "id": "OVUSY2-BDjss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords('Probably my all time favourite movie is Pursuit of Happyness')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hqxIZSSlDjvj",
        "outputId": "82dd464b-15d1-4fb3-a1d2-0bd8a2080316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Probably   time favourite movie  Pursuit  Happyness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_emojis(data):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                            \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text) # no emoji\n",
        "\n"
      ],
      "metadata": {
        "id": "TFqbHRwuDjz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VKdVR3oHc-Q",
        "outputId": "5d77f10d-1dc1-42b6-9eac-c8e9b69e30fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2"
      ],
      "metadata": {
        "id": "lui4d8ZyHkZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "print(emoji.demojize('Python is ðŸ‘'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PHHP0QAHUur",
        "outputId": "6106104d-868f-43e1-d67c-ef22f287d270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is :thumbs_up:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_text=\"In my professional journey, Iâ€™ve worked on exciting projects that combined data analysis and creative problem-solving ðŸš€. At Deloitte, I helped reduce the attrition rate by 26% ðŸ“‰ through insights-driven dashboards built with SQL and Tableau, empowering HR teams to make smarter decisions ðŸ¤. At Target ðŸŽ¯, I contributed to key deliverables using Python, Hive, and Excel, showcasing my knack for building efficient workflows and insightful data models ðŸ“Š. Additionally, I spearheaded a project to analyze driver performance ðŸš›, optimizing metrics like distance covered and drive time, which led to a 15% cost saving in the next quarter ðŸ’°. My passion for turning data into actionable insights fuels my drive to keep delivering impactful results! ðŸŒŸ\"\n",
        "\n",
        "emoji.demojize(emoji_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "q-5nM7_MDj2i",
        "outputId": "d33fadda-d679-43be-8ab3-29b0cdaa6dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In my professional journey, Iâ€™ve worked on exciting projects that combined data analysis and creative problem-solving :rocket:. At Deloitte, I helped reduce the attrition rate by 26% :chart_decreasing: through insights-driven dashboards built with SQL and Tableau, empowering HR teams to make smarter decisions :handshake:. At Target :bullseye:, I contributed to key deliverables using Python, Hive, and Excel, showcasing my knack for building efficient workflows and insightful data models :bar_chart:. Additionally, I spearheaded a project to analyze driver performance :articulated_lorry:, optimizing metrics like distance covered and drive time, which led to a 15% cost saving in the next quarter :money_bag:. My passion for turning data into actionable insights fuels my drive to keep delivering impactful results! :glowing_star:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "x9hqXoWNyFaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method1: Using split"
      ],
      "metadata": {
        "id": "P4QDTzxuJyMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenization\n",
        "sent1= ' I am going to delhi'\n",
        "sent1.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkLhy4CNJ1DM",
        "outputId": "263478c0-f1b8-4ced-cff1-a45c6d4ed310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenization\n",
        "sent2=' I am going to Delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
        "sent2.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFXaePuYJ1GQ",
        "outputId": "01c1f53a-17ef-4842-c45c-3f168db47c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' I am going to Delhi',\n",
              " ' I will stay there for 3 days',\n",
              " \" Let's hope the trip to be great\"]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problems with split function\n",
        "sent3= 'I am going to Delhi!' # '!' treated as part of Delhi\n",
        "sent3.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpLBuI5IJ1Iz",
        "outputId": "dc334a38-9356-46f5-d8d3-f3364eb4d786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'Delhi!']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent4='Where do think I should go? I have 3 day holiday'\n",
        "sent4.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiFTfPAGJ1Lz",
        "outputId": "f1337fe2-fe25-4020-df0a-fbc61f7b63d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Where do think I should go? I have 3 day holiday']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2:Regular Expression"
      ],
      "metadata": {
        "id": "RKl7xEqCKimS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sent3 = 'I am going to delhi!'\n",
        "tokens=re.findall(\"[\\w']+\",sent3)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "342L5WboJ1OJ",
        "outputId": "a835f19c-3e6d-467f-8733-01661881b971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy=\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\n",
        "sentences=re.compile('[.!?] ').split(dummy)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYpHOPZ8K6T2",
        "outputId": "57487c7b-cedd-41c2-b5f4-7a9478c6c998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
              " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book\",\n",
              " 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged',\n",
              " 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 3: NLTK Library Function"
      ],
      "metadata": {
        "id": "B1UfCEk6LL1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ1WQyiOyaTB",
        "outputId": "9b260ffb-7414-46e5-c5c2-0824d7a0679e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7LPUDwdorZi"
      },
      "outputs": [],
      "source": [
        "dummy=\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKFt51FByJTL",
        "outputId": "a188a6db-63b0-4f27-d518-77adfa588941"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents =sent_tokenize(dummy)\n",
        "sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E5N1LeXyPbK",
        "outputId": "efe2d8b7-6b33-45bd-dce4-b75af61ebdab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lorem Ipsum is simply dummy text of the printing and typesetting industry.',\n",
              " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\",\n",
              " 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
              " 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in sents:\n",
        "  print(word_tokenize(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpj9hFX5yT90",
        "outputId": "56ee6c2b-9c5f-4a21-be1d-b40a016a863b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.']\n",
            "['Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.']\n",
            "['It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.']\n",
            "['It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 4: Spacy"
      ],
      "metadata": {
        "id": "UJjfzq1aLaUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "GrF8gX2oy0Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1=nlp(sent1)\n",
        "doc2=nlp(sent2)\n",
        "doc3=nlp(sent3)\n",
        "doc4=nlp(sent4)"
      ],
      "metadata": {
        "id": "2-hE1WXULiAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc4:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC9uTmAPLwMS",
        "outputId": "aa959c3f-d8eb-4fd4-c6ec-bea6374e10c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Where\n",
            "do\n",
            "think\n",
            "I\n",
            "should\n",
            "go\n",
            "?\n",
            "I\n",
            "have\n",
            "3\n",
            "day\n",
            "holiday\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc3:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouZM-4gpLyif",
        "outputId": "2f0aeb57-d18b-4e28-e6a2-2d59731cd1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "am\n",
            "going\n",
            "to\n",
            "delhi\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "BJOrXA8MOK_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
      ],
      "metadata": {
        "id": "LTudypR--rcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "gR7ayb7IL8VJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps=PorterStemmer()\n",
        "def stem_words(text):\n",
        "  return \" \".join([ps.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "hVr18Er9OOA5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 'walk walks walking walked'\n",
        "stem_words(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3v3erMj7OVbo",
        "outputId": "5fef09d7-aaea-4799-c9ff-c195a0df22dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'walk walk walk walk'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegexpStemmer class\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
      ],
      "metadata": {
        "id": "4v4_Ma2g-8-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "7VweQ8nU--lJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "CbuqIbdr--vN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "id": "tEHQnSFe_Lml",
        "outputId": "73982d26-ea76-499a-fd62-2a4115c5881e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "id": "lRjGl1Nf_Lrw",
        "outputId": "8f7b9ebc-1e54-4e50-da3f-451207449f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Snowball Stemmer\n",
        " It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
      ],
      "metadata": {
        "id": "yUCKcy23_TZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "xgKjoJTE_Ut6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "yQuJc8Pf_Uxr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
        "for word in words:\n",
        "    print(word+\"---->\"+snowballsstemmer.stem(word))"
      ],
      "metadata": {
        "id": "mGLQ7ggJ_VKj",
        "outputId": "32872ec8-e816-43d9-e0ba-d5c01d52e118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem(\"fairly\"),ps.stem(\"sportingly\")"
      ],
      "metadata": {
        "id": "A_m7PqoX_lqi",
        "outputId": "035b6b90-7116-40b3-b488-3886589cbe4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer.stem(\"fairly\"),snowballsstemmer.stem(\"sportingly\")"
      ],
      "metadata": {
        "id": "hG3WgLre_lvo",
        "outputId": "24e610bf-597e-435d-98b7-cb5b85cbe978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer.stem('goes')"
      ],
      "metadata": {
        "id": "7JgbVmWw_0Yt",
        "outputId": "a07ab2f7-19d6-410a-bf60-aaa99a68282a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('goes')"
      ],
      "metadata": {
        "id": "ctcS2Mun_6Il",
        "outputId": "aa641779-0928-4ec8-cbdf-ad45dd3f5e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "xT8mX0IWPKzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from  nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "sentence='  He was running and eating at the same time. He has bad habit of swimming after playing long hours in the sun'\n",
        "punctuations='?:!.,;'\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "  if word in punctuations:\n",
        "    sentence_words.remove(word)\n",
        "sentence_words\n",
        "\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "  print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9QNxEG-Oa05",
        "outputId": "f695c357-8e3e-43a0-ca75-7d6cae0de062"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Lemma               \n",
            "He                  He                  \n",
            "was                 be                  \n",
            "running             run                 \n",
            "and                 and                 \n",
            "eating              eat                 \n",
            "at                  at                  \n",
            "the                 the                 \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 have                \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swim                \n",
            "after               after               \n",
            "playing             play                \n",
            "long                long                \n",
            "hours               hours               \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "sun                 sun                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "mOA3XXLbPumJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\",pos='v')"
      ],
      "metadata": {
        "id": "3jip_S52BwL5",
        "outputId": "f96585a5-a386-4f50-8fca-d2cc7df7ad3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ],
      "metadata": {
        "id": "9FW4janGB9wd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "id": "45TWEHTsCApV",
        "outputId": "7a544050-b12b-48a9-a9f2-503a3cd46632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"goes\",pos='v')"
      ],
      "metadata": {
        "id": "C089wt2tCDFF",
        "outputId": "9e7a617f-f4bd-4ffe-d2e4-cce13cf1dd5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"fairly\",pos='v'),lemmatizer.lemmatize(\"sportingly\")"
      ],
      "metadata": {
        "id": "fnZCm5MlCFRB",
        "outputId": "dc4284bc-6bc2-4f82-f5d7-36d1f4278e75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SmJxzg0kCGvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}